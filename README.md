# az-data-lab

<!-- PROJECT LOGO -->
<br />
<div align="center">
  <a href="">
    <img src="docs/logo.jpg" alt="Logo" width="300" height="150">
  </a>
</div>

## Goal & purpose
Ziel dieses Projekts ist es, eine **end-to-end Data Engineering-Umgebung auf Azure** aufzubauen ‚Äì  Datenuploads in eine SQL-Datenbank in einem Data, Aufbau von Pipelines f√ºr die Transformation der Daten, Integration mit Databricks f√ºr die Steuerung von Data Lineage und Anwendung der Governance. Es wurde bewusst darauf geachtet, m√∂glichst **kosteng√ºnstig** innerhalb eines Azure Free Trials zu arbeiten.

## Azure account erstellen
- Azure Free Trial mit 200 USD Budget: https://azure.microsoft.com/free/
- Account kann mit einer privaten E-Mail-Adresse erstellt werden (z.‚ÄØB. Gmail)
- Kreditkarte wird zur Verifizierung ben√∂tigt, es entstehen jedoch keine Kosten, solange die kostenlosen Limits nicht √ºberschritten werden. Belastung von 1 USD zur Verifizierung.

### Overview
F√ºr die Erstellung der Grundstruktur der Azure-Diensten wird folgendes Schema angewendet:
![Process](docs/scope-levels.png)
- **Management Group** (*optional*): Gruppieren mehrerer Subscriptions in einer Organisation. Wird hier nicht angewendet.
- **Subscription** (*Free Trial*): Repr√§sentiert ein Abrechnungs- und Ressourcenkonto, um innerhalb  Ressourcen und deren Limits zu definieren
- **Resource Group** (*z.B. rg-dataproject1*): Logische Sammlung von Ressourcen, wie SQL Datenbank, Storage Account, Databricks Workspace, Data Factory usw.
- **Ressourcen**: Einzelkomponenten zur Verwendung im Projekt
  - Azure SQL Database ‚Üí f√ºr strukturierte Daten
  - Storage Account (ADLS Gen2) ‚Üí f√ºr Dateien, Delta Lake
  - Azure Databricks ‚Üí f√ºr Analyse, Transformation, ML
  - Key Vault ‚Üí optional f√ºr sichere Passw√∂rter & Secrets

Dies erm√∂glicht eine saubere Trennung und Verwaltung der Cloud-Infrastruktur.

## Vorbereitungen f√ºr zwei Use Cases:
### Szenario 1: Cloud based

<details>
<summary>Klicke hier, um den Ablauf anzuzeigen</summary>

<br>

- Service: Azure SQL Database
- Tier: Free/Basic (vCore serverless) f√ºr Testzwecke
- WICHTIG: Bei der Authentifizierung "SQL authentication" ausw√§hlen (nicht Entra ID oder AAD)
- Benutzername/Passwort wird direkt beim Erstellen gesetzt
- Zugriff √ºber Firewall-Regel f√ºr eigene IP und Databricks-IP gew√§hrleisten

### Tools

- **ODBC Driver 18 for SQL Server**  
  Download: https://learn.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server

- **Python 3.11+**
- **Git** f√ºr Versionierung
- **Azure CLI** f√ºr Skripting: https://learn.microsoft.com/en-us/cli/azure/install-azure-cli

### Python Libraries

```bash
pip install pyodbc pandas sqlalchemy python-dotenv
```

- `sqlalchemy` ‚Üí f√ºr Verbindung zur DB
- `pandas` ‚Üí f√ºr Datenverarbeitung
- `pyodbc` ‚Üí f√ºr direkte SQL-Verbindung
- `dotenv` ‚Üí f√ºr Umgebungsvariablen

---

### Setup Workflow

Ziel: End-to-End-Datenfluss in Azure mit Python aufbauen:

1. **Datenbank erstellen**
2. **Verbindung testen** (`db_connect.py`)
3. **Tabelle erstellen** (`db_create_table.py`)
4. **CSV-Daten laden** (`db_upload_data.py`)
5. **Verifizierung √ºber Azure Portal oder Python**

#### Access

- Das Skript `db_connect.py` testet die Verbindung zur SQL-Datenbank mit Umgebungsvariablen aus `.env`
- Nutzt `pyodbc` f√ºr direkten SQL-Zugriff

```python
conn = pyodbc.connect(
    f"DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}"
)
```

#### Create Table

- `db_create_table.py` enth√§lt das `CREATE TABLE`-Statement f√ºr die Tabelle `charging_stats`
- Nutzt `pyodbc` um die Tabelle direkt in der Datenbank anzulegen

#### Load Data

- `db_upload_data.py` l√§dt eine CSV-Datei (`charging_data.csv`) in die zuvor erstellte Tabelle
- Nutzt `pandas` + `sqlalchemy` um bulk insert durchzuf√ºhren

```python
df = pd.read_csv("charging_data.csv")
df.to_sql("charging_stats", con=engine, if_exists="append", index=False)
```

#### Verify Deployment

- Im Azure Portal: √∂ffne die SQL-Datenbank ‚Üí **Query Editor (Preview)**
- Melde dich mit SQL-Login an
- F√ºhre z.‚ÄØB. aus:

```sql
SELECT TOP 10 * FROM charging_stats;
```

- Alternativ: Kontrolle auch m√∂glich via Python/Notebook (`pd.read_sql(...)`)

</details>

---


### Szenario 2: On Prem basiert

<details>
<summary>Klicke hier, um den Ablauf anzuzeigen</summary>

<br>

![Scenario2](docs/scenario2.jpg)

1. **Einrichten der Self-hosted Integration Runtime (SHIR):**
   - Installation und Konfiguration der SHIR auf dem lokalen Server, um eine sichere Verbindung zwischen der lokalen Umgebung und Azure Data Factory herzustellen.

2. **Erstellen eines Linked Services f√ºr die lokale SQL Server-Datenbank:**
   - Konfiguration der Verbindungsdetails zur lokalen SQL Server-Datenbank in Azure Data Factory, einschlie√ülich Servername, Datenbankname und Authentifizierungsinformationen.

3. **Erstellen eines Linked Services f√ºr Azure Data Lake Storage Gen2:**
   - Einrichtung der Verbindung zu Azure Data Lake Storage Gen2 durch Angabe des Speicherortnamens und der Authentifizierungsdetails.

4. **Erstellen einer Pipeline in Azure Data Factory:**
   - Zusammenstellung einer Pipeline mit einer Copy Data-Aktivit√§t, die die Daten von der lokalen SQL Server-Datenbank in den Azure Data Lake Storage Gen2 √ºbertr√§gt.

5. **Konfigurieren der Copy Data-Aktivit√§t:**
   - Festlegen der Quelle (lokale SQL Server-Datenbank) und des Ziels (Azure Data Lake Storage Gen2), Auswahl der zu kopierenden Tabellen oder Daten und Festlegung des Datenformats f√ºr die Speicherung im Data Lake.

6. **Ver√∂ffentlichen und Ausf√ºhren der Pipeline:**
   - Speichern und Ver√∂ffentlichen der erstellten Pipeline und anschlie√üendes Starten der Pipeline, um den Daten√ºbertragungsprozess zu initiieren.

7. **√úberwachen der Pipeline-Ausf√ºhrung:**
   - Verfolgung des Fortschritts und √úberpr√ºfung auf Fehler oder Warnungen w√§hrend der Ausf√ºhrung der Pipeline √ºber die Monitoring-Funktion in Azure Data Factory.

</details>

---

### Szenario 3: Databricks & Unity Catalog Setup mit Azure SQL & Storage üöÄ

<details>
<summary>Klicke hier, um den Ablauf anzuzeigen</summary>

<br>

Dieses Kapitel beschreibt Schritt f√ºr Schritt, wie eine Azure SQL-Datenbank mit Databricks (Premium Tier) verkn√ºpft wird, um Daten zu lesen und Unity Catalog zu testen. Ideal f√ºr erste praktische Erfahrungen in einer Cloud-Datenumgebung.

---

## Voraussetzungen

- Azure Subscription mit Budget (kein Free Trial)
- Zugriff auf das Azure-Portal mit Adminrechten
- Azure SQL-Datenbank & SQL Server
- Azure Storage Account mit **Hierarchical Namespace (HNS)** aktiviert
- Databricks Free Trial mit **Premium Tier** gew√§hlt
- Du bist der Azure **Entra Admin** / Directory Admin

---

## Ressourcen im Projekt

- **SQL Server:** `xxx-cmdb`
- **SQL Datenbank:** `xxx-cmdb/cmdb`
- **Storage Account:** `xxxxdatalake`
- **Virtual Network:** `nw-xxxxxx-xxxx`

---

## Schritt-f√ºr-Schritt Anleitung

### 1. Storage Account pr√ºfen (Unity Catalog Voraussetzung)

> Unity Catalog ben√∂tigt einen Data Lake Storage mit aktiviertem **Hierarchical Namespace** (ADLS Gen2).

- Gehe in Azure zum Storage `xxxxdatalake`
- Navigiere zu **Configuration**
- **Hierarchical namespace = Enabled**
    - Falls **nicht aktiviert**: neuen Storage Account erstellen (mit HNS!)

---

### 2. Databricks Workspace erstellen (Premium Tier)

- Gehe im Azure-Portal zu "Azure Databricks"
- W√§hle:
  - **Pricing Tier:** Premium
  - **Region:** identisch mit SQL & Storage (z. B. Switzerland North)
  - **Resource Group:** gleich wie f√ºr andere Ressourcen
- Deployment starten

---

### 3. Unity Catalog einrichten

- Navigiere im Databricks-Workspace zu **Admin Settings > Unity Catalog**
- Klicke **Enable Unity Catalog**
- Folge dem Wizard:
  - Erstelle einen **Metastore**
  - Verkn√ºpfe den Storage Account `xxxxdatalake`
  - Lege dich als **Metastore-Admin** fest

---

### 4. Verbindung zur SQL-Datenbank herstellen (JDBC)

- Firewall-Regel im SQL Server: IP von Databricks zulassen
- Stelle sicher, dass **SQL Authentication** aktiviert ist
- Beispiel-Notebook:

```python
jdbc_url = "jdbc:sqlserver://xxx-xxxx.database.windows.net:xxxx;database=cmdb"
properties = {
  "user": "dein_user",
  "password": "dein_passwort",
  "driver": "com.microsoft.sqlserver.jdbc.SQLServerDriver"
}

df = spark.read.jdbc(url=jdbc_url, table="dbo.deine_tabelle", properties=properties)
df.display()
```

---

### 5. Daten in Unity Catalog schreiben (Delta Table)

```python
df.write.format("delta").saveAsTable("mein_catalog.mein_schema.cmdb_daten")
```

Damit sind die Daten in einer verwalteten Tabelle verf√ºgbar und per SQL, ML oder BI nutzbar.

---

## Wichtige Hinweise

| Thema                    | Empfehlung                                   |
|-------------------------|-----------------------------------------------|
| Identit√§t               | Unity Catalog setzt Azure Entra ID voraus     |
| Kostenkontrolle         | Single-Node Cluster & Auto-Termination aktiv |
| Rechte & Sicherheit     | Verwende Gruppen f√ºr Metastore-Zugriffe       |
| Datenquellen            | F√ºr JDBC-Zugriffe: IP whitelisten & Auth sichern |
| Housekeeping            | Clusternutzung √ºberwachen & ungenutzte Ressourcen l√∂schen |

---

## Fazit

Mit diesem Setup kannst du:

- SQL-Daten in Databricks verarbeiten
- Unity Catalog aktiv testen (Metastore + Data Governance)
- Erste Pipelines oder ML-Anwendungen bauen

</details>

---



