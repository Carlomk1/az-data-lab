# az-data-lab

<!-- PROJECT LOGO -->
<br />
<div align="center">
  <a href="">
    <img src="docs/logo.jpg" alt="Logo" width="300" height="150">
  </a>
</div>

## Goal & purpose
Ziel dieses Projekts ist es, eine **end-to-end Data Engineering-Umgebung auf Azure** aufzubauen ‚Äì  Datenuploads in eine SQL-Datenbank in einem Data, Aufbau von Pipelines f√ºr die Transformation der Daten, Integration mit Databricks f√ºr die Steuerung von Data Lineage und Anwendung der Governance. Es wurde bewusst darauf geachtet, m√∂glichst **kosteng√ºnstig** innerhalb eines Azure Free Trials zu arbeiten.

## Azure account
- Azure Free Trial mit 200 USD Budget: https://azure.microsoft.com/free/
- Account kann mit einer privaten E-Mail-Adresse erstellt werden (z.‚ÄØB. Gmail)
- Kreditkarte wird zur Verifizierung ben√∂tigt, es entstehen jedoch keine Kosten, solange die kostenlosen Limits nicht √ºberschritten werden. Belastung von 1 USD zur Verifizierung.

### Overview
F√ºr die Erstellung der Grundstruktur der Azure-Diensten wird folgendes Schema angewendet:
![Process](docs/scope-levels.png)
- Management Group (*optional*): Gruppieren mehrerer Subscriptions in einer Organisation. Wird hier nicht angewendet.
- Subscription (*Free Trial*): Repr√§sentiert ein Abrechnungs- und Ressourcenkonto, um innerhalb  Ressourcen und deren Limits zu definieren
- Resource Group (*z.B. rg-dataproject1*): Logische Sammlung von Ressourcen, wie SQL Datenbank, Storage Account, Databricks Workspace, Data Factory usw.
- Ressourcen: Einzelkomponenten zur Verwendung im Projekt
  - Azure SQL Database ‚Üí f√ºr strukturierte Daten
  - Storage Account (ADLS Gen2) ‚Üí f√ºr Dateien, Delta Lake
  - Azure Databricks ‚Üí f√ºr Analyse, Transformation, ML
  - Key Vault ‚Üí optional f√ºr sichere Passw√∂rter & Secrets

Dies erm√∂glicht eine saubere Trennung und Verwaltung der Cloud-Infrastruktur.

## Vorbereitungen f√ºr zwei Use Cases:
### Szenario 1: Cloud based

<details>
<summary>Klicke hier, um den Ablauf anzuzeigen</summary>

<br>

- Service: Azure SQL Database
- Tier: Free/Basic (vCore serverless) f√ºr Testzwecke
- WICHTIG: Bei der Authentifizierung "SQL authentication" ausw√§hlen (nicht Entra ID oder AAD)
- Benutzername/Passwort wird direkt beim Erstellen gesetzt
- Zugriff √ºber Firewall-Regel f√ºr eigene IP und Databricks-IP gew√§hrleisten

### Tools

- **ODBC Driver 18 for SQL Server**  
  Download: https://learn.microsoft.com/en-us/sql/connect/odbc/download-odbc-driver-for-sql-server

- **Python 3.11+**
- **Git** f√ºr Versionierung
- **Azure CLI** f√ºr Skripting: https://learn.microsoft.com/en-us/cli/azure/install-azure-cli

### Python Libraries

```bash
pip install pyodbc pandas sqlalchemy python-dotenv
```

- `sqlalchemy` ‚Üí f√ºr Verbindung zur DB
- `pandas` ‚Üí f√ºr Datenverarbeitung
- `pyodbc` ‚Üí f√ºr direkte SQL-Verbindung
- `dotenv` ‚Üí f√ºr Umgebungsvariablen

---

### Setup Workflow

Ziel: End-to-End-Datenfluss in Azure mit Python aufbauen:

1. **Datenbank erstellen**
2. **Verbindung testen** (`db_connect.py`)
3. **Tabelle erstellen** (`db_create_table.py`)
4. **CSV-Daten laden** (`db_upload_data.py`)
5. **Verifizierung √ºber Azure Portal oder Python**

#### Access

- Das Skript `db_connect.py` testet die Verbindung zur SQL-Datenbank mit Umgebungsvariablen aus `.env`
- Nutzt `pyodbc` f√ºr direkten SQL-Zugriff

```python
conn = pyodbc.connect(
    f"DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}"
)
```

#### Create Table

- `db_create_table.py` enth√§lt das `CREATE TABLE`-Statement f√ºr die Tabelle `charging_stats`
- Nutzt `pyodbc` um die Tabelle direkt in der Datenbank anzulegen

#### Load Data

- `db_upload_data.py` l√§dt eine CSV-Datei (`charging_data.csv`) in die zuvor erstellte Tabelle
- Nutzt `pandas` + `sqlalchemy` um bulk insert durchzuf√ºhren

```python
df = pd.read_csv("charging_data.csv")
df.to_sql("charging_stats", con=engine, if_exists="append", index=False)
```

#### Verify Deployment

- Im Azure Portal: √∂ffne die SQL-Datenbank ‚Üí **Query Editor (Preview)**
- Melde dich mit SQL-Login an
- F√ºhre z.‚ÄØB. aus:

```sql
SELECT TOP 10 * FROM charging_stats;
```

- Alternativ: Kontrolle auch m√∂glich via Python/Notebook (`pd.read_sql(...)`)

</details>

---


### Szenario 2: On Prem basiert

<details>
<summary>Klicke hier, um den Ablauf anzuzeigen</summary>

<br>

![Scenario2](docs/scenario2.jpg)

1. **Einrichten der Self-hosted Integration Runtime (SHIR):**
   - Installation und Konfiguration der SHIR auf dem lokalen Server, um eine sichere Verbindung zwischen der lokalen Umgebung und Azure Data Factory herzustellen.

2. **Erstellen eines Linked Services f√ºr die lokale SQL Server-Datenbank:**
   - Konfiguration der Verbindungsdetails zur lokalen SQL Server-Datenbank in Azure Data Factory, einschlie√ülich Servername, Datenbankname und Authentifizierungsinformationen.

3. **Erstellen eines Linked Services f√ºr Azure Data Lake Storage Gen2:**
   - Einrichtung der Verbindung zu Azure Data Lake Storage Gen2 durch Angabe des Speicherortnamens und der Authentifizierungsdetails.

4. **Erstellen einer Pipeline in Azure Data Factory:**
   - Zusammenstellung einer Pipeline mit einer Copy Data-Aktivit√§t, die die Daten von der lokalen SQL Server-Datenbank in den Azure Data Lake Storage Gen2 √ºbertr√§gt.

5. **Konfigurieren der Copy Data-Aktivit√§t:**
   - Festlegen der Quelle (lokale SQL Server-Datenbank) und des Ziels (Azure Data Lake Storage Gen2), Auswahl der zu kopierenden Tabellen oder Daten und Festlegung des Datenformats f√ºr die Speicherung im Data Lake.

6. **Ver√∂ffentlichen und Ausf√ºhren der Pipeline:**
   - Speichern und Ver√∂ffentlichen der erstellten Pipeline und anschlie√üendes Starten der Pipeline, um den Daten√ºbertragungsprozess zu initiieren.

7. **√úberwachen der Pipeline-Ausf√ºhrung:**
   - Verfolgung des Fortschritts und √úberpr√ºfung auf Fehler oder Warnungen w√§hrend der Ausf√ºhrung der Pipeline √ºber die Monitoring-Funktion in Azure Data Factory.

</details>

üöÄ




